{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3534fb10-fd82-4d08-8f2a-6728d6170e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported python libraries\n",
    "# To import, run \"conda install -c conda-forge <package1> <package2>\" in the Anaconda prompt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy import linalg\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e139196e-7dfc-4131-94f4-fd167a742c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load NVIDIA data from exported files\n",
    "# Function to load NVIDIA data from exported files\n",
    "def load_nvidia_data(base_dir='data'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : Base directory where data files are stored\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary containing loaded data\n",
    "    \"\"\"\n",
    "    # Define directory paths\n",
    "    csv_dir = f'{base_dir}'\n",
    "    train_dir = f'{base_dir}/npy/train'\n",
    "    test_dir = f'{base_dir}/npy/test'\n",
    "    \n",
    "    # Loads CSV data\n",
    "    technical_data = pd.read_csv(f'{csv_dir}/nvidia_technical_data.csv', index_col=0, parse_dates=True, date_format=\"%d/%m/%Y\")\n",
    "    normalized_data = pd.read_csv(f'{csv_dir}/nvidia_normalized_data.csv', index_col=0, parse_dates=True, date_format=\"%d/%m/%Y\")\n",
    "    ceflann_data = pd.read_csv(f'{csv_dir}/nvidia_ceflann_data.csv', index_col=0, parse_dates=True, date_format=\"%d/%m/%Y\")\n",
    "    \n",
    "    # Loads NumPy arrays for training and testing\n",
    "    X_train = np.load(f'{train_dir}/nvidia_X_train.npy')\n",
    "    y_train = np.load(f'{train_dir}/nvidia_y_train.npy')\n",
    "    train_dates = np.load(f'{train_dir}/nvidia_train_dates.npy', allow_pickle=True)\n",
    "    train_prices = np.load(f'{train_dir}/nvidia_train_price_data.npy')\n",
    "    \n",
    "    # Load main test data\n",
    "    X_test = np.load(f'{test_dir}/nvidia_X_test.npy')\n",
    "    y_test = np.load(f'{test_dir}/nvidia_y_test.npy')\n",
    "    test_dates = np.load(f'{test_dir}/nvidia_test_dates.npy', allow_pickle=True)\n",
    "    test_prices = np.load(f'{test_dir}/nvidia_price_data.npy')\n",
    "    \n",
    "    # Try to load multiple test weeks if available\n",
    "    test_weeks = []\n",
    "    test_weeks_info = np.load(f'{base_dir}/npy/nvidia_test_weeks_info.npy', allow_pickle=True)\n",
    "    \n",
    "    # Load each test week\n",
    "    for i in range(len(test_weeks_info)):\n",
    "        try:\n",
    "            week_data = {\n",
    "                'X': np.load(f'{test_dir}/nvidia_X_test_week_{i}.npy'),\n",
    "                'y': np.load(f'{test_dir}/nvidia_y_test_week_{i}.npy'),\n",
    "                'dates': np.load(f'{test_dir}/nvidia_test_dates_week_{i}.npy', allow_pickle=True),\n",
    "                'prices': np.load(f'{test_dir}/nvidia_price_data_week_{i}.npy'),\n",
    "                'info': test_weeks_info[i]\n",
    "            }\n",
    "            test_weeks.append(week_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load test week {i}: {e}\")\n",
    "    \n",
    "    # Loads latest info\n",
    "    with open(f'{csv_dir}/nvidia_info.txt', 'r') as f:\n",
    "        latest_info = f.read().split(',')\n",
    "        latest_date = latest_info[0]\n",
    "        latest_close = float(latest_info[1])\n",
    "        test_week_length = int(latest_info[2]) if len(latest_info) > 2 else 5\n",
    "        num_test_weeks = int(latest_info[3]) if len(latest_info) > 3 else 0\n",
    "    \n",
    "    return {\n",
    "        'technical_data': technical_data,\n",
    "        'normalized_data': normalized_data,\n",
    "        'ceflann_data': ceflann_data,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'train_dates': train_dates,\n",
    "        'train_prices': train_prices,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'test_dates': test_dates,\n",
    "        'test_prices': test_prices,\n",
    "        'test_weeks': test_weeks,\n",
    "        'test_weeks_info': test_weeks_info,\n",
    "        'latest_date': latest_date,\n",
    "        'latest_close': latest_close,\n",
    "        'test_week_length': test_week_length,\n",
    "        'num_test_weeks': num_test_weeks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8e2bae-087d-470e-b09f-4353a6934000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements the CEFLANN model with ELM learning\n",
    "class CEFLANN:\n",
    "    \"\"\"\n",
    "    Computational Efficient Functional Link Artificial Neural Network\n",
    "    \n",
    "    This implementation follows the paper \"A hybrid stock trading framework integrating \n",
    "    technical analysis with machine learning techniques\" by Dash & Dash (2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, expansion_order=5, regularization=0.01):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        expansion_order(int) : The order of expansion for the functional expansion block\n",
    "        regularization(float) : Regularization parameter for the ELM learning\n",
    "        \"\"\"\n",
    "        self.expansion_order = expansion_order\n",
    "        self.regularization = regularization\n",
    "        self.output_weights = None\n",
    "        self.expansion_params = None\n",
    "        \n",
    "    def _functional_expansion(self, X):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : Input feature matrix with shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        expanded_X : Expanded input pattern array\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Number of expanded features = original features + expansion order\n",
    "        n_expanded = n_features + self.expansion_order\n",
    "        \n",
    "        # Initializes the expanded feature matrix\n",
    "        expanded_X = np.zeros((n_samples, n_expanded))\n",
    "        expanded_X[:, :n_features] = X\n",
    "        \n",
    "        \n",
    "        if self.expansion_params is None:\n",
    "            # For each order i, initializes parameters a_i0 and a_ij (j is from 1 to n_features)\n",
    "            self.expansion_params = []\n",
    "            for i in range(self.expansion_order):\n",
    "                # Initialize bias term a_i0\n",
    "                a_i0 = np.random.uniform(-1, 1)\n",
    "                \n",
    "                # Initialize weights a_ij for each feature\n",
    "                a_ij = np.random.uniform(-1, 1, size=n_features)\n",
    "                \n",
    "                self.expansion_params.append((a_i0, a_ij))\n",
    "        \n",
    "        # Applies functional expansion for each order\n",
    "        for i in range(self.expansion_order):\n",
    "            a_i0, a_ij = self.expansion_params[i]\n",
    "            \n",
    "            # Calculates weighted sum of input features\n",
    "            weighted_sum = a_i0 + np.dot(X, a_ij)\n",
    "            \n",
    "            # Applies a tanh activation function\n",
    "            expanded_X[:, n_features + i] = np.tanh(weighted_sum)\n",
    "            \n",
    "        return expanded_X\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : Training feature matrix of shape (n_samples, n_features)\n",
    "        y : Target values array of shape (n_samples,)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : Returns self\n",
    "        \"\"\"\n",
    "        # Applies functional expansion to transform input features\n",
    "        expanded_X = self._functional_expansion(X)\n",
    "        \n",
    "        # Adds the bias to the expanded input\n",
    "        n_samples = expanded_X.shape[0]\n",
    "        bias_col = np.ones((n_samples, 1))\n",
    "        M = np.hstack((bias_col, expanded_X))\n",
    "        \n",
    "        # Uses regularized least squares (Ridge Regression) for output weights        \n",
    "        n_cols = M.shape[1]\n",
    "        MtM = np.dot(M.T, M)\n",
    "        reg_term = self.regularization * np.eye(n_cols)\n",
    "        inverse_term = np.linalg.inv(MtM + reg_term)\n",
    "        MP_inverse = np.dot(inverse_term, M.T)\n",
    "        \n",
    "        # Calculates output weights\n",
    "        self.output_weights = np.dot(MP_inverse, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : Test feature matrix of shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : Array of predicted values\n",
    "        \"\"\"\n",
    "        # Applies functional expansion to transform input features\n",
    "        expanded_X = self._functional_expansion(X)\n",
    "        \n",
    "        # Adds bias to expanded input\n",
    "        n_samples = expanded_X.shape[0]\n",
    "        bias_col = np.ones((n_samples, 1))\n",
    "        M = np.hstack((bias_col, expanded_X))\n",
    "        \n",
    "        y_pred = np.dot(M, self.output_weights)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : Test feature matrix array\n",
    "        y : Target values array\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        mse : Mean squared error\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fcbd56-4269-40d5-b332-5a75a3552467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model on the training dataset\n",
    "def train_model(X_train, y_train, expansion_order=5, regularization=0.01):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : Training features\n",
    "    y_train : Training targets\n",
    "    expansion_order : Order of expansion for functional expansion block\n",
    "    regularization : Regularization parameter for ELM learning\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    trained_model : Trained CEFLANN model\n",
    "    training_time : Time taken to train the model\n",
    "    \"\"\"\n",
    "    model = CEFLANN(expansion_order=expansion_order, regularization=regularization)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return model, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb6f591-5358-4674-88ee-f2a23bba0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simulation_week(data, start_date='2025-03-10', end_date='2025-03-14'):\n",
    "    \"\"\"\n",
    "    Extract data for the simulation week (March 24-28, 2025)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : Dictionary containing loaded data\n",
    "    start_date : Start date of simulation week (default: March 24, 2025)\n",
    "    end_date : End date of simulation week (default: March 28, 2025)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary containing data for the simulation week\n",
    "    \"\"\"\n",
    "    # Convert string dates to datetime objects\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Try to find the simulation week in the test weeks\n",
    "    simulation_week = None\n",
    "    \n",
    "    for week in data['test_weeks']:\n",
    "        week_start = pd.to_datetime(week['dates'][0])\n",
    "        week_end = pd.to_datetime(week['dates'][-1])\n",
    "        \n",
    "        # Check if there's overlap with our simulation period\n",
    "        if (week_start <= end and week_end >= start):\n",
    "            simulation_week = week\n",
    "            break\n",
    "    \n",
    "    # If we couldn't find the exact week, use the most recent test week\n",
    "    if simulation_week is None:\n",
    "        print(f\"Warning: Could not find exact simulation week. Using most recent test week as a proxy.\")\n",
    "        simulation_week = data['test_weeks'][-1]\n",
    "    \n",
    "    # Extract the technical indicators for this week from the full dataset\n",
    "    week_dates = pd.to_datetime(simulation_week['dates'])\n",
    "    technical_data_week = data['technical_data'].loc[week_dates[0]:week_dates[-1]]\n",
    "    \n",
    "    return {\n",
    "        'X': simulation_week['X'],\n",
    "        'y': simulation_week['y'],\n",
    "        'dates': simulation_week['dates'],\n",
    "        'prices': simulation_week['prices'],\n",
    "        'technical_data': technical_data_week,\n",
    "        'info': simulation_week['info'] if 'info' in simulation_week else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64f040e-54fc-4f9e-96f9-a73e9bb94163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_ceflann(data, target_week=None):\n",
    "    \"\"\"\n",
    "    Prepare data in the format required by the CEFLANN model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : Dictionary containing loaded data\n",
    "    target_week : Optional specific week to prepare (if None, uses main test data)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary containing prepared data for CEFLANN model\n",
    "    \"\"\"\n",
    "    if target_week:\n",
    "        X = target_week['X']\n",
    "        y = target_week['y']\n",
    "        dates = target_week['dates']\n",
    "        prices = target_week['prices']\n",
    "    else:\n",
    "        X = data['X_test']\n",
    "        y = data['y_test']\n",
    "        dates = data['test_dates']\n",
    "        prices = data['test_prices']\n",
    "    \n",
    "    # Ensure X and y are properly formatted as numpy arrays\n",
    "    X = np.array(X).astype(float)\n",
    "    y = np.array(y).astype(float)\n",
    "    \n",
    "    # Check for any NaN values and handle them\n",
    "    if np.isnan(X).any():\n",
    "        print(\"Warning: Input features contain NaN values. Replacing with zeros.\")\n",
    "        X = np.nan_to_num(X, nan=0.0)\n",
    "    \n",
    "    if np.isnan(y).any():\n",
    "        print(\"Warning: Target values contain NaN values. Replacing with mean.\")\n",
    "        mean_y = np.nanmean(y)\n",
    "        y = np.nan_to_num(y, nan=mean_y)\n",
    "    \n",
    "    return {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'dates': dates,\n",
    "        'prices': prices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6997e1d-592a-4d3c-b53e-7c7b71011fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_output_to_trading_signal(predictions, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Convert model predictions to trading signals (0 = downtrend, 1 = uptrend)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : Model output predictions (values in range 0-1)\n",
    "    threshold : Threshold value for classification (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Numpy array of trend signals (0 or 1)\n",
    "    \"\"\"\n",
    "    return (predictions > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab74daf-813f-4393-b1a2-4a63c34f5119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Latest date: 2025-03-14 00:00:00\n",
      "Latest NVIDIA close price: $121.67\n",
      "Training data shape: X=(1031, 6), y=(1031,)\n",
      "Testing data shape: X=(5, 6), y=(5,)\n",
      "Number of test weeks available: 54\n"
     ]
    }
   ],
   "source": [
    "# Load all NVIDIA data\n",
    "nvidia_data = load_nvidia_data()\n",
    "\n",
    "# Print summary of loaded data\n",
    "print(f\"Data loaded successfully. Latest date: {nvidia_data['latest_date']}\")\n",
    "print(f\"Latest NVIDIA close price: ${nvidia_data['latest_close']:.2f}\")\n",
    "print(f\"Training data shape: X={nvidia_data['X_train'].shape}, y={nvidia_data['y_train'].shape}\")\n",
    "print(f\"Testing data shape: X={nvidia_data['X_test'].shape}, y={nvidia_data['y_test'].shape}\")\n",
    "print(f\"Number of test weeks available: {len(nvidia_data['test_weeks'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27c8796-4a7f-461e-b683-8b3c1f3bdf49",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m in \u001b[0;35mget_loc\u001b[0m\n    return self._engine.get_loc(casted_key)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mindex.pyx:167\u001b[0m in \u001b[0;35mpandas._libs.index.IndexEngine.get_loc\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mindex.pyx:196\u001b[0m in \u001b[0;35mpandas._libs.index.IndexEngine.get_loc\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m in \u001b[0;35mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[1;36m in \u001b[1;35mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;36m\n",
      "\u001b[1;31mKeyError\u001b[0m\u001b[1;31m:\u001b[0m Timestamp('2025-03-10 00:00:00')\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6798\u001b[0m in \u001b[0;35mget_slice_bound\u001b[0m\n    slc = self.get_loc(label)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[1;36m in \u001b[1;35mget_loc\u001b[1;36m\n\u001b[1;33m    raise KeyError(key) from err\u001b[1;36m\n",
      "\u001b[1;31mKeyError\u001b[0m\u001b[1;31m:\u001b[0m Timestamp('2025-03-10 00:00:00')\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[0;32mIn[9], line 2\u001b[0m\n    simulation_week = get_simulation_week(nvidia_data)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 38\u001b[0m in \u001b[0;35mget_simulation_week\u001b[0m\n    technical_data_week = data['technical_data'].loc[week_dates[0]:week_dates[-1]]\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m in \u001b[0;35m__getitem__\u001b[0m\n    return self._getitem_axis(maybe_callable, axis=axis)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexing.py:1411\u001b[0m in \u001b[0;35m_getitem_axis\u001b[0m\n    return self._get_slice_axis(key, axis=axis)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexing.py:1443\u001b[0m in \u001b[0;35m_get_slice_axis\u001b[0m\n    indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop, slice_obj.step)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6662\u001b[0m in \u001b[0;35mslice_indexer\u001b[0m\n    start_slice, end_slice = self.slice_locs(start, end, step=step)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6879\u001b[0m in \u001b[0;35mslice_locs\u001b[0m\n    start_slice = self.get_slice_bound(start, \"left\")\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6801\u001b[0m in \u001b[0;35mget_slice_bound\u001b[0m\n    return self._searchsorted_monotonic(label, side)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6733\u001b[0m in \u001b[0;35m_searchsorted_monotonic\u001b[0m\n    return self.searchsorted(label, side=side)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\base.py:1352\u001b[0m in \u001b[0;35msearchsorted\u001b[0m\n    return algorithms.searchsorted(\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\algorithms.py:1329\u001b[1;36m in \u001b[1;35msearchsorted\u001b[1;36m\n\u001b[1;33m    return arr.searchsorted(value, side=side, sorter=sorter)  # type: ignore[arg-type]\u001b[1;36m\n",
      "\u001b[1;31mTypeError\u001b[0m\u001b[1;31m:\u001b[0m '<' not supported between instances of 'str' and 'Timestamp'\n"
     ]
    }
   ],
   "source": [
    "# Get simulation week data (March 24-28, 2025)\n",
    "simulation_week = get_simulation_week(nvidia_data)\n",
    "print(\"\\nSimulation Week Data:\")\n",
    "print(f\"- Dates: {simulation_week['dates'][0]} to {simulation_week['dates'][-1]}\")\n",
    "print(f\"- Number of trading days: {len(simulation_week['dates'])}\")\n",
    "print(f\"- Features shape: X={simulation_week['X'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895de8e0-f013-4483-8dfa-441945ed4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CEFLANN model\n",
    "prepared_data = prepare_data_for_ceflann(nvidia_data, simulation_week)\n",
    "print(\"\\nData prepared for CEFLANN model:\")\n",
    "print(f\"- Input features shape: {prepared_data['X'].shape}\")\n",
    "print(f\"- Target values shape: {prepared_data['y'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1eb2c3-3cfa-4008-8f2d-205e1c97ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the simulation week data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pd.to_datetime(simulation_week['dates']), simulation_week['prices'], marker='o')\n",
    "plt.title('NVIDIA Stock Prices - Simulation Week (March 24-28, 2025)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
